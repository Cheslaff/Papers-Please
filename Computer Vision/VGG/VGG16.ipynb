{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VGG-16 PyTorch\nMayhem flows...<br>\nInspired with AlexNet succes and with understanding of how important factor of depth is for neural networks VGG networks (yep, there are multiple configurations) were presented to the world in 2014.","metadata":{}},{"cell_type":"markdown","source":"## Part I. Quick Overview & Goal Setting.\nThis network doesn't resolve vanishing gradient issue, so it's a bit hard to train (oh yay, it's deep).<br>\nI don't expect something crazy here (oh and yeah, I'll use ImageNet subset with 100 classes, so it's hard to expect something great here).\nOriginal VGG had been trained for weeks, however my Kaggle quota is not limitless;)<br>\nGraph you see below is an example of VGG network architecture.<br>\n\n<img src=\"https://habrastorage.org/webt/9l/mi/6-/9lmi6-8zya2_tcw8sg3gctnmtv0.png\" width=100%><br>\nVGG architecture is straightforward (no skip-connections or inception blocks), but it's still important to clarify how such depth could be achieved.<br>\nThe key is stacking multiple convolutional layers with small kernel size (3x3 is the minimal kernel size that stores spatial data, like center, right, left, top, bottom)<br>\nIt has its benefits:<br>\n- More non-linearities (ReLUs used). After each convolutional layer we use ReLU, so instead of using ReLU once after a layer with big kernel size we use it twice or more between these stacked convolutional layers.\n- Same receptive field. We compensate kernel size with quantity of layers.\n- Less parameters usedl. While 2 3x3x3 convolutions use 2*27=54 parameters 1 7x7x3 uses 147 parameters.\n\nPractically, that's it. Once again, nothing crazy or mindblowing, just smarter usage of convolutional layers.<br>\n\n## Data.\nAs mentioned earlier, I'll use ImageNet100 subset of RGB images.","metadata":{}},{"cell_type":"markdown","source":"## Part II. Data Loading","metadata":{}},{"cell_type":"code","source":"!pip install torchmetrics\n!pip install torchinfo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchmetrics import Accuracy\nfrom torchinfo import summary\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = Path(\"/kaggle/input/imagenet100\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_transform = transforms.Compose([\n    transforms.Resize(size=(256, 256)),\n    transforms.RandomCrop(size=(224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"part_0 = ImageFolder(root_dir / \"train.X1\", transform=data_transform)\npart_1 = ImageFolder(root_dir / \"train.X2\", transform=data_transform)\npart_2 = ImageFolder(root_dir / \"train.X3\", transform=data_transform)\npart_3 = ImageFolder(root_dir / \"train.X4\", transform=data_transform)\n\ntrain_dataset = ConcatDataset([part_0, part_1, part_2, part_3])\ntest_dataset = ImageFolder(root_dir / \"val.X\", transform=data_transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset, test_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n                              num_workers=os.cpu_count())\n\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n                             num_workers=os.cpu_count())\n\ntrain_dataloader, test_dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image_batch, label_batch in train_dataloader:\n    image = image_batch[0]\n    label = label_batch[0]\n\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(image.permute(1, 2, 0));\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part III. Model","metadata":{}},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.convolutional = nn.Sequential(\n            # 1st \"block\"\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            # 2nd \"block\"\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            # 3rd \"block\"\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            # 4th \"block\"\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            # 5th \"block\"\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Flatten()\n        )\n\n        self.dense = nn.Sequential(\n            nn.Linear(in_features=7*7*512, out_features=4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(in_features=4096, out_features=4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(in_features=4096, out_features=100)\n        )\n\n    def forward(self, x):\n        return self.dense(self.convolutional(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg = VGG16().to(device)\nsummary(vgg, input_size=(64, 3, 224, 224))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(model: nn.Module,\n               dataloader: torch.utils.data.DataLoader,\n               loss_fn: nn.Module,\n               optimizer: torch.optim.Optimizer,\n               device: torch.device):\n    accuracy_fn = Accuracy(\"multiclass\", num_classes=100).to(device)\n    running_loss, running_acc = 0, 0\n\n    model.train()\n    \n    for image_batch, label_batch in dataloader:\n        image_batch = image_batch.to(device)\n        label_batch = label_batch.to(device)\n\n        logits = model(image_batch)\n        activated_pred = torch.softmax(logits, dim=1).argmax(dim=1)\n\n        batch_accuracy = accuracy_fn(activated_pred, label_batch)\n        batch_loss = loss_fn(logits, label_batch)\n        running_loss += batch_loss\n        running_acc += batch_accuracy\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        nn.utils.clip_grad_norm_(vgg.parameters(), max_norm=10)\n        optimizer.step()\n\n    running_loss /= len(dataloader)\n    running_acc /= len(dataloader)\n\n    return running_loss, running_acc\n\n\ndef test_step(model: nn.Module,\n              dataloader: torch.utils.data.DataLoader,\n              loss_fn: nn.Module,\n              device: torch.device):\n    accuracy_fn = Accuracy(\"multiclass\", num_classes=100).to(device)\n    running_loss, running_acc = 0, 0\n\n    model.eval()\n    \n    with torch.inference_mode():\n        for image_batch, label_batch in dataloader:\n            image_batch = image_batch.to(device)\n            label_batch = label_batch.to(device)\n    \n            logits = model(image_batch)\n            activated_pred = torch.softmax(logits, dim=1).argmax(dim=1)\n    \n            batch_accuracy = accuracy_fn(activated_pred, label_batch)\n            batch_loss = loss_fn(logits, label_batch)\n            running_loss += batch_loss\n            running_acc += batch_accuracy\n    \n        running_loss /= len(dataloader)\n        running_acc /= len(dataloader)\n    return running_loss, running_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=vgg.parameters(), momentum=0.9, weight_decay=0.0005, lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 20\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n\nresults = {\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"test_loss\": [],\n    \"test_acc\": []\n}\n\nwriter = SummaryWriter()\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_step(vgg, train_dataloader, loss_fn, optimizer, device)\n    test_loss, test_acc = test_step(vgg, test_dataloader, loss_fn, device)\n\n    print(f\"EPOCH: {epoch} | \"\n          f\"tr_loss: {train_loss} | tr_acc: {train_acc}\"\n          f\"ts_loss: {test_loss} | ts_acc: {test_acc}\"\n         )\n\n    results[\"train_loss\"].append(train_loss)\n    results[\"train_acc\"].append(train_acc)\n    results[\"test_loss\"].append(test_loss)\n    results[\"test_acc\"].append(test_acc)\n\n    writer.add_scalar('Loss/train', train_loss, epoch)\n    writer.add_scalar('Loss/test', test_loss, epoch)\n    writer.add_scalar('Accuracy/train', train_acc, epoch)\n    writer.add_scalar('Accuracy/test', test_acc, epoch)\n\n    scheduler.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}