<p align="center"><H1 align="center">Papers, Please!ðŸ“„</H1></p><br>
<p align="center">Implementing famous research papers to better understand the ideas they propose.</p><br>
<p align="center"><img src="https://web.stanford.edu/class/cs25/images/transformer_seal.png" width=20% style="border-radius:80%"></p>

<p align="center">Road to Karpathy/GPT2!</p>

### ðŸŸ¢ **Stage 1: Historical & Conceptual Foundations (NLP-heavy)**

1. **Neural Probabilistic Language Model** â€“ Bengio et al., 2003 ðŸ†•
    
2. **Word2Vec (SGNS)** â€“ Mikolov et al., 2013
    
3. **GloVe** â€“ Pennington et al., 2014
    
4. **LSTM** â€“ Hochreiter & Schmidhuber, 1997
    
5. **Seq2Seq LSTM** â€“ Sutskever et al., 2014

---

### ðŸŸ¡ **Stage 2: Encoder-Decoder Era & Generative Intuitions**

5. **CharRNN** â€“ Karpathy, 2015 ðŸ†•  
    _Pure code, clean intuition for next-token generation_
    
7. **Stacked Denoising Autoencoders** â€“ Vincent et al., 2010  
    _Intro to unsupervised pretraining_
    
8. **Neural Style Transfer** â€“ Gatys et al., 2015  
    _Understanding CNN representations visually_
    
9. **DRAW: RNN-VAE with Attention** â€“ Gregor et al., 2015 ðŸ†•  
    _Fascinating iterative generation. Attention without Transformers._
    

---

### ðŸŸ  **Stage 3: Specialized / Deep Ideas (Some Vision, Mostly NLP + Graphs + Gen)**

10. **U-Net** â€“ Ronneberger et al., 2015  
    _Not just for CV: understand the encoder-decoder idea fully_
    
11. **WaveNet** â€“ van den Oord et al., 2016  
    _Pure autoregressive, no attention. Great GPT precursor_
    
12. **Neural GPU** â€“ Kaiser & Sutskever, 2015 ðŸ†•  
    _Train neural nets to learn addition/multiplication â€” very GPTish before GPT._
    
13. **Fast Weights (Ba et al., 2016)** ðŸ†•  
    _Temporary memory via outer products. Think of it as a form of early attention. Wildly underexplored._
    
14. **HyperNetworks** â€“ Ha et al., 2016 ðŸ†•  
    _Neural nets that generate the weights of other neural nets._
    

---

### ðŸ”µ **Stage 4: Graphs, Recurrence, Evolutionary Ideas**

15. **Graph Convolutional Networks (GCN)** â€“ Kipf & Welling, 2016  
    _Build intuition on non-Euclidean DL_
    
16. **Relational Reasoning (Relation Networks)** â€“ Santoro et al., 2017 ðŸ†•  
    _Add structure-aware logic to conv outputs â€“ amazing for question answering._
    
17. **ES-MNIST (Neural Architecture Search)** â€“ Real et al., 2017 ðŸ†•  
    _Evolutionary approaches to net design. (Nice contrast to hand-designed ResNet/VGG.)_
    
18. **Sparse Evolutionary Training** â€“ Mocanu et al., 2018 ðŸ†•  
    _Connections are removed/added during training. Sheds light on trainable topology._

---


<p align="center"><img src="https://www.svgrepo.com/show/444064/legal-license-mit.svg" width=10%></p>
<p align="center" style="color:grey;">KurwAI labs. 2024-2024</p>
